\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt]{scrartcl}

\usepackage[margin=1in]{geometry}

\usepackage[utf8]{inputenc}

\usepackage{graphicx}

\usepackage{float}

\usepackage{amssymb,amsmath}

\usepackage{colortbl}

\title{SimPL \\ Simple Pong Learner}
\author{David Lettier}
\date{\today}

\pdfinfo{%
  /Title    (Simpl: Simple Pong Learner)
  /Author   (David Lettier)
  /Creator  (David Lettier)
  /Producer (David Lettier)
  /Subject  (machine learning)
  /Keywords (evolutionary genetic algorithm)
}

\begin{document}
\maketitle

\textit{Note: this write up stub is to be included in the master's thesis proposal, CISC 7902X project proposal, and the CISC 7900X semester write up.}

\section{Introduction}

\textbf{Sim}ple \textbf{P}ong \textbf{L}earner or SimPL is an asymmetric autonomous pong clone with one paddle and one ball. SimPL is web based comprised of web based technologies: HTML5, JavaScript, CSS, AJAX, MySQL, and PHP. At the time of this writing, SimPL can be viewed at http://www.lettier.com/simpl/. The paddle in SimPL is controlled by a feed-forward neural network. The neural network's weights are tuned via a genetic algorithm. 

The focus for SimPL was to learn about and to cultivate a genetic algorithm capable of tuning parameters with respect to a fitness landscape thereby producing an optimum solution to a given parameter space. The genetic algorithm developed for SimPL will be used as a basis for a genetic algorithm needed to solve a harder problem of tuning a 3D physics engine. 

\section{Implementation}

\subsection{Arena}

The arena for SimPL resides in a browser window. Four transparent walls reside at the top, right, bottom, and left of the screen. The arena contains a ball and paddle with the paddle affixed to the far left of screen and the ball originating from the far right of the screen. See Figure \ref{fig:arena}.

\begin{figure}[H]  
  \centering
  \includegraphics[width=.9\textwidth]{figures/arena.png}
  \caption{Here you see the SimPL arena containing the paddle and ball.}
  \label{fig:arena}
\end{figure}

\subsection{Ball}

The ball is a physics based dynamic object. Its starting position and starting velocity magnitude are the same at the start of every round\footnote{A round is defined as the time from when the ball is launched from its starting position to either the time at which the ball leaves the left side of the arena or at the time in which the ball's velocity magnitude drops below $100$.}. Just before the beginning of a round, a random angle in the range $[135^\circ,225^\circ]$ is chosen as the ball's starting angle. See Figure \ref{fig:ball}. The ball's position is managed by the physics engine which responds to any collisions against the arena walls and/or the paddle.

\begin{figure}[H]  
  \centering
  \includegraphics[width=.7\textwidth]{figures/ball.png}
  \caption{Here you see the ball's dynamic physics properties.}
  \label{fig:ball}
\end{figure}

If the ball collides with the left wall, the round is over. Otherwise, if the ball collides with the top, right, or bottom wall, the ball is bounced back into the arena via its angle-of-reflection based on its collision angle-of-incidence. Collisions with the paddle work in the same fashion where the ball is bounced back via its angle-of-reflection based on its collision angle-of-incidence. See Figure \ref{fig:angle}.

\begin{figure}[H]  
  \centering
  \includegraphics[width=.5\textwidth]{figures/angle.png}
  \caption{Here you see the ball's collision angle-of-incidence $\theta_i$ and its angle-of-reflection $\theta_r$.}
  \label{fig:angle}
\end{figure}

Each collision the ball makes reduces its velocity magnitude. Let $m$ denote the ball's velocity magnitude. The formula used is $m = m - (m * 50\%)$. Once the ball's velocity magnitude drops below $100$ the round is over.

\pagebreak

\subsection{Paddle}

The paddle is a physics based dynamic object that has a fixed velocity angle of either $90^\circ$ or $270^\circ$. See Figure \ref{fig:paddle}. Its starting position as well as its starting velocity magnitude are the same at the start of every round. 

\begin{figure}[H]  
  \centering
  \includegraphics[width=.3\textwidth]{figures/paddle.png}
  \caption{Here you see the paddle's dynamic physics properties.}
  \label{fig:paddle}
\end{figure}

The paddle's direction and speed is regulated by the output of the neural network. The output of the neural network is in the range $[-1,1]$. A neural network output of $0$ results in the paddle not moving from its current position. A neural network output in the range of $(0,1]$ results in the paddle traveling up by some percentage of its starting velocity magnitude. A neural network output in the range of $[-1,0)$ results in the paddle traveling down by some percentage of it its starting velocity magnitude. For example, say the neural network output is $0.68$. The paddle's velocity angle would be set to $90^\circ$ and its velocity magnitude would be set to $m_{new}=0.68*m_{initial}$.

Collisions can occur for the paddle between the top wall, the bottom ball, and the ball. Collision with either the top or bottom wall results in the paddle's top or bottom being placed just before the wall. Collision with the ball results in no change of movement for the paddle---the paddle merely continues moving as it was before the collision occurred with the ball.

\subsection{Physics Engine}

All dynamic and static objects are registered with the physics engine before the first round. Once every draw loop of the simulation, the physics engine tests for collisions between dynamic objects and other dynamic objects and between dynamic objects and static objects. Those dynamic objects that are found to be colliding with either another dynamic object and/or static object are flagged as such and their collisions are handled accordingly. For those dynamic objects that are not colliding, their position is updated based on their velocity.

\subsection{Neural Network}

The neural network is a feed-forward neural network that contains one input layer consisting of six input nodes, one hidden layer consisting of 5 hidden nodes, and one output layer consisting of one output node. Each threshold input to the hidden nodes and the output node is included among the weights of the network and thus are optimized or tuned via the genetic algorithm. All total, there are 41 weights ($(6+1)*5+(5+1)*1=41$) contained in the network. See Figure \ref{fig:nn}. Thus, there are 41 genes per genome in the genetic algorithm's population. All output from the hidden nodes and the output node are run through a sigmoid, hyperbolic-tangent-activation function $\left ( tanh(x)=\frac{e^{2x}-1}{e^{2x}+1} \right )$ resulting in an output range of $[-1,1]$.

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/nn.png}
  \caption{Here you see the neural network as constructed in SimPL.}
  \label{fig:nn}
\end{figure}

Input to the neural network is normalized generating three unit vectors: from the balls's center to the paddle's center, the ball's velocity, and from the window's origin to the paddle's center. See Figure \ref{fig:unit_vectors}. These three unit vectors are broken down into their components resulting in six inputs to the neural network. See Figure \ref{fig:nn_input}. 

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/unit_vectors.png}
  \caption{Here you see the three input vectors to the neural network. Note that these vectors are normalized thereby turning them into unit vectors.}
  \label{fig:unit_vectors}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/nn_input.png}
  \caption{Here you see the normalized input to the neural network.}
  \label{fig:nn_input}
\end{figure}

\pagebreak

\subsection{Genetic Algorithm}

Instead of using back-propagation to train the weights of the neural network, a genetic algorithm is used to optimize or rather tune the weights of the neural network. The genetic algorithm consists of a population of genomes with each having a fitness property and an array of genes. For SimPL, the genes represent a solution of weights to be used in the neural network. Each genome is fitness evaluated by a fitness function. As the genomes evolve over generations to produce fitter genomes, the neural network becomes increasingly accurate at classifying what the paddle should do (move up, stay still, or move down) based on the state of the ball and the paddle. 

The genetic algorithm contains four operators that work to produce fitter generations during the creation of a new population. The operators include: the elitism operator, the selection operator, the crossover operator, and the mutation operator. Initially, the genetic algorithm creates a population of genomes. These initial genomes have zero fitness and a fix number of genes. Each gene in every genome is given a random value sampled from a uniform distribution coinciding with some valid range. The genes are the input parameters to whatever mechanism the genetic algorithm is working to optimize. In the case of SimPL, the mechanism is the neural network and the genes or parameters---of each genome---represent the 41 weights of the neural network. Each gene/parameter/weight has a valid range of $[-1,1]$. 

Once every genome in the population has been evaluated by the fitness function, the genetic algorithm constructs a new population/generation from the now old population/generation. First, the elitism operator selects the $n$ fittest genomes from the old generation. These elite genomes are allowed to survive intact (however their fitnesses are reset to zero) and are placed into the new generation. Second, the genetic algorithm enters into a loop creating new genomes via the crossover operator and the mutation operator until an entirely new generation has been created. This new generation goes on to be evaluated as their predecessors were and the cycle repeats until some termination criteria. See Figure \ref{fig:ga}.

\begin{figure}[H]
\begin{equation*}
\boxed{
\begin{aligned}
Begin \ GA: & \\
&Generate \ population \ P_0. \\
& While \ not \ terminate: \\
& \quad\quad Evalute \ population \ P_i. \\
& \quad\quad Create \ empty \ populutation \ P_{i+1}. \\
& \quad\quad P_{i+1} \longleftarrow Select \ n \ fittest \ from \ P_i. \\
& \quad\quad While \ |P_{i+1}| \ < \  population \ size: \\
& \quad\quad\quad\quad If \ in \ sequence: \\
& \quad\quad\quad\quad\quad\quad g_1,g_2 \longleftarrow \ Select \ genomes \ from \ P_i. \\
& \quad\quad\quad\quad\quad\quad o_1,o_2 \longleftarrow \ Crossover( g_1, g_2 ). \\
& \quad\quad\quad\quad\quad\quad P_{i+1} \longleftarrow \ Mutate( o_1, o_2 ). \\
& \quad\quad\quad\quad Else: \\
& \quad\quad\quad\quad\quad\quad g_1,g_2 \longleftarrow \ Select \ genomes \ from \ P_i. \\
& \quad\quad\quad\quad\quad\quad P_{i+1} \longleftarrow \ Crossover( g_1, g_2 ). \\
& \quad\quad\quad\quad\quad\quad P_{i+1} \longleftarrow \ Mutate( g_1, g_2 ). \\
& \quad\quad P_{i} \longleftarrow P_{i+1}\\
\end{aligned}
}
\end{equation*}
\caption{Here you see the basic genetic algorithm.}
\label{fig:ga}
\end{figure}

\vspace{3mm}

The selection operator uses roulette selection where the probability of some genome being selected is proportional to their fitness. Roulette selection and roulette selection with rank fitness was experimented with as outlined later. The crossover operator takes two selected genomes to produce offspring where the offspring have some combination of their parents' genes. SimPL uses one-point crossover throughout. The mutation operator takes a selected genome and mutates its genes using various means. These means of mutation where experimented with as outlined later. Crossover and mutation can be carried out in sequence or can be done separately with one operator not interfering the other operator's offspring. This was experimented with as outlined later.

Crossover and mutation are not guaranteed to always occur as the genetic algorithm produces new genomes from the old genomes. Rather, crossover and mutation occur based on some probability. Of course if the crossover and mutation probabilities are both set to $1$ then yes they always occur. Before the genetic algorithm is initialized, the crossover and mutation probabilities are set before hand. These probabilities can change over time or can remain static throughout the run of the genetic algorithm. Different static settings and self-adaptation of the probabilities were experimented with as outlined later. 

The fitness function evaluates each genome in the current population based on some fitness criteria. The fitness criteria coincides with finding the optimum solution to the parameter space of the mechanism the genetic algorithm is producing fitter and fitter genomes/solutions to. For SimPL, the parameter space is the weights to the neural network. Each genome represents a solution or point in the weights/parameter space. An optimal solution in the weights space will make the neural network always give a correct classification as to what movement the paddle must make based on the ball's and the paddle's current state. This of course will result in the paddle always following the ball or in other words, the paddle will never let the ball leave the left side of the screen. Thus the fitness criteria for SimPL could include how much or how well the paddle follows the ball and/or how many times the paddle hits the ball. Different fitness criteria were experimented with as outlined later. 

% GA loop
% crossover
% mutation
% observed rates
% perturbation
% Gaussian mutation
% whole versus gene-by-gene mutation
% sequential operators versus separate
% self-adaptation via tracking progress
% partial credit fitness

\subsection{Database Manager}

The database manager interfaces with a remote MySQL server either asynchronously or synchronously. Experiment data is recorded to the MySQL database as the experiment is carried out. Additionally, each generation produced by the genetic algorithm is logged to the database. Upon visiting the SimPL site, the last generation produced can be loaded into the genetic algorithm thereby allowing the simulation to pick up where it left off last.  

\section{Platforms}

The screen size or rather the browser window size has a significant influence over the fitnesses of the genomes being evaluated. While the arena fits to whatever size the browser window is, the size of the paddle and the ball do not directly change in proportion to the window size. Thus, a small browser window gives the paddle less screen real estate to cover in comparison to a large browser window. For example, imagine a browser window size with a height as large as the paddle. Here the paddle can never move and therefore always follows the ball and always hits the ball thus resulting in an erroneously high fitness. Therefore it is important to note what platform SimPL was run on when discussing experimental results---especially when comparing experimental results with one another. Care was taken to run experiments on the same platform when the experiments would be directly compared to one another. 

The laptop's browser window size is $\sim70\%$ the size of the desktop browser window size (see Figure \ref{fig:platform_sizes}) and thus higher fitnesses are observed when comparing identical runs of the genetic algorithm performed on the laptop versus the desktop. On the desktop the paddle has $992-220=772$ pixels to traverse while on the laptop the paddle has $681-200=481$ pixels to traverse.

\begin{figure}[H]  
  \centering
  \includegraphics[width=.666\textwidth]{figures/platform_sizes.png}
  \caption{Here you see the laptop screen superimposed onto the desktop screen.}
  \label{fig:platform_sizes}
\end{figure}

%1526.3410497 / 2161.125632628  = 0.706271318

\subsection{Desktop}

Google Chrome version number 26.0.1410.63 was used on the desktop. Screen resolution was $1920\times1080$. Browser window size (both reported and actual) was 1920 pixels wide by 992 pixels tall. Paddle size reported (by the browser) was 50 pixels wide by 200 pixels tall but the actual size was 55 pixels wide by 220 pixels tall. Ball size reported (by the browser) was 50 pixels wide by 50 pixels tall but the actual size was 57 pixels wide by 57 pixels tall.

\subsection{Laptop}

Google Chrome version number 30.0.1599.114 was used on the laptop. Screen resolution was $1366\times768$. Browser window size (both reported and actual) was 1366 pixels wide by 681 pixels tall. Paddle size reported (by the browser) was 50 pixels wide by 200 pixels tall and the actual size was the same. Ball size reported (by the browser) was 50 pixels wide by 50 pixels tall and the actual size was the same. 

\section{Experimental Designs}

\subsection{Experiment one: use of a full and partial fitness credit fitness function.}

Experiment one centered around the use of a fitness function that would give full and partial credit based on the behavior of the neural network and thus the paddle. The genetic algorithm was run for 100 generations with each generation's average fitness being recorded. Once every 10th generation, the current population's top performing genome was set aside. These 10 every-10th-generation-top-performers were run for five rounds each with the time they kept the ball-in-play being recorded per each one of their individual five rounds. Once an every-10th-generation-top-performer was done with their five rounds, the average of their ball-in-play time was calculated and recorded. The platform used was the desktop. The genetic algorithm parameters used are listed in Table \ref{tab:exp1}.

When evaluating a genome from the population, if its phenotype (the paddle's observable characteristics) followed the ball it would be given a positive partial fitness credit of $0.1$. If the phenotype managed to collide with the ball, the genome was given a full fitness credit of $1$. However, if the phenotype moved away from the ball it was given negative partial fitness credit of $-0.1$. Lastly, if the phenotype didn't move at all (while the ball moved) it was given $0$ fitness. If the total fitness was less than zero, for any genome evaluated, the total fitness was set to zero. 

To facilitate tracking the paddle's movements in relation to the ball's movements, the absolute difference in heights between the paddle's center and the ball center's were recorded every draw loop into an array. Once the genome was ready to be evaluated, this array of differences was analyzed linearly in pairs, that is, $A[i]$ was compared with $A[i+1]$. If $A[i]>A[i+1]$ this indicated that the paddle's center was moving closer to the ball's center and thus the genome was awarded a positive partial credit fitness. If $A[i]<A[i+1]$ this indicated that the paddle's center was moving away from the ball's center and thus the genome was awarded a negative partial credit fitness. If $A[i]=A[i+1]$ this indicated that the paddle's center was neither moving toward nor away from the ball's center and thus the genome was awarded $0$ fitness. A special case for $A[i]=A[i+1]$ was that if $A[i]=0$ and thus $A[i+1]=0$ as well, the paddle was awarded a positive partial fitness as the paddle's center was dead center with the ball's center and therefore the paddle was directly in the path of the ball which is ultimately the goal---that is, the paddle should always matches its center with the ball's center thereby always preventing the ball from leaving the arena.

The hypothesis for using a full and partial fitness credit schema was that every new generation produced would have genomes that at least performed somewhat better than their predecessors at the optimal behaviors. Those optimal behaviors being following the ball and hitting the ball. Originally, only hitting the ball was going to be the fitness criteria. However, early generations may never hit the ball and thus would have zero fitness. Genomes that at least followed the ball could have been erroneously discarded due to having zero fitness. By giving partial credit for at least following the ball, it was hypothesized that it would seed early generations with promising genomes or rather promising solutions to the parameter space. Picture a classroom of students taking a test. Upon evaluation, it is either all or nothing credit per question and no student finds the solution to any problem. In other words, every student received a zero. This would give the teacher no information as to the quality of the students at least in terms of comparing them to one another. However, if partial credit is given for getting part of the solution correct, then this would give at least some information as to who performed well among the students. In other words, it was hypothesized that by using a finer grained fitness function, there would be some information gained as to the fitness of one genome compared to another (aiding elitism and the selection process) versus no information gained using only a coarsely grained fitness function resulting in every genome having a fitness of zero after being evaluated.

\begin{table}[H]
\footnotesize
\begin{tabular}{ >{\columncolor[gray]{0.8}} l | l }
\hline
Number of Elite Offspring                                            & 2                                                         \\ \hline
Roulette Selection using Actual Fitness                              & True                                                      \\ \hline
Roulette Selection using Rank Fitness                                & False                                                     \\ \hline
Crossover Probability                                                & 0.7                                                       \\ \hline
Crossover Type                                                       & One Point Crossover                                       \\ \hline
Mutation Probability                                                 & 0.1                                                       \\ \hline
Mutation Scope                                                       & Gene Level                                                \\ \hline
Mutation Step                                                        & Uniform Dist. Sample $\in[0,1]$ * Max Perturbation        \\ \hline
Max Perturbation                                                     & 0.3                                                       \\ \hline
Sequential Crossover and Mutation                                    & True                                                      \\ \hline
Offspring not Crossed/Mutated Allowed in New Generation              & True                                                      \\ \hline
Fitness Credit for Hitting the Ball                                  & 1.0                                                       \\ \hline
Fitness Credit for Following the Ball                                & 0.1                                                       \\ \hline
Fitness Credit for not Following the Ball                            & -0.1                                                      \\ \hline
\end{tabular}
\caption{Here you see the genetic algorithm parameters for experiment one.}
\label{tab:exp1}
\end{table}

\subsection{Experiment two: use of a simplified fitness function, use of rank fitness in selection, higher crossover probability, mutation probability based on the number of genes per genome, and a Gaussian distribution sample mutation step.}

% Talk about wide gaps of fitness. Giving poor performers at least some chance of being selected for crossover and mutation. Keeping the population diverse and avoiding early convergence due to the top performers just swapping the same genes over and over after x generations.

Experiment two had a simplified fitness function, used a genome's rank fitness during selection, increased the crossover probability, based the mutation probability on the number of genes per genome, and used Gaussian distribution sampling as the mutation step. The genetic algorithm was run for 100 generations with each generation's average fitness being recorded. Once every 10th generation, the current population's top performing genome was set aside. These 10 every-10th-generation-top-performers were run for five rounds each with the time they kept the ball-in-play being recorded per each one of their individual five rounds. Once an every-10th-generation-top-performer was done with their five rounds, the average of their ball-in-play time was calculated and recorded. The platform used was the desktop. The genetic algorithm parameters used are listed in Table \ref{tab:exp2}.

Going from awarding full and partial credit fitness based on two behaviors to only awarding a fitness credit of $1$ if the paddle was in the path of the ball per every draw loop, the fitness function was simplified. Here the paddle doesn't necessarily have to be dead center to the ball but rather the paddle's top must be at or above the ball's top while at the same time the paddle's bottom has to be at or below the ball's bottom. See Figure \ref{fig:simple_fit_func}. The reasoning behind this was that if the paddle were to always be in the path of the ball then it would always hit the ball and thus the paddle would never let the ball leave the arena or in other words the paddle would exhibit the optimum desired behavior. Therefore, this fitness function correctly evaluates the paddle's (or rather the genome phenotype's) behavior in relation to ball's movement through out any round.

\begin{figure}[H]  
  \centering
  \includegraphics[width=.5\textwidth]{figures/simple_fit_func.png}
  \caption{Here you see the simplified fitness function graphically where the paddle on the left is gaining fitness (as indicated by the green-hued bar) while the paddle on the right is gaining no fitness (as indicated by the purple-hued bar).}
  \label{fig:simple_fit_func}
\end{figure}

Instead of using the actual fitness of any particular genome in the population to determine its probability of being selected during the roulette wheel selection process, a genome's rank fitness was used to determine its probability of being selected. It was observed during early runs of the genetic algorithm that after evaluation the population had wide gaps of fitness. The genomes with zero or relatively low fitness had absolutely little to no chance of being selected for crossover and mutation while the genomes with a relatively high fitness had a high chance of being selected for crossover and mutation. These early top performers (possibly due more to chance than skill) were continuously selected, crossed, and mutated generation after generation. Population diversity dwindled thereby causing the population to converge too early (due to an ever narrowing search of the fitness landscape) resulting in poor performance on behalf of the genetic algorithm. Thus it was hypothesized that by using rank fitness instead of actual fitness to determine a genome's probability of being selected, population diversity would remain sufficient generation after generation thereby avoiding early convergence. 

Rank fitness is a genome's fitness according to their index in the population after the population is sorted in increasing order of fitness. After sorting the population, genome one is given a fitness of one, genome two is given a fitness of two and so on and so forth until genome $n$ is given a fitness of $n$ or rather the population size. See Figure \ref{fig:roulette}. Now all genomes have a better chance of being selected to under go crossover and/or mutation thereby keeping the population diversity high and thus keeping the search scope of the fitness landscape large resulting in better performance of the genetic algorithm.   

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/roulette.png}
  \caption{Here you see actual fitness versus rank fitness probabilities of being selected.}
  \label{fig:roulette}
\end{figure}

Crossover probability was increased from $0.7$ to $0.8$. This of course would result in more observed crossovers being generated as new populations were created. Since crossover produces an offspring solution  somewhere between its parents in the fitness landscape, it was hypothesized that by increasing the crossover probability the local search capability of the genetic algorithm would also increase.

Based on empirical studies performed by others, setting the mutation probability to $\frac{1}{n}$---where $n$ is the number of genes per genome---is a sufficient enough amount of random search to allow the genetic algorithm to escape local maximum in the fitness landscape \cite{predictive_models}. The reasoning behind $\frac{1}{n}$ is that per mutation on a gene-by-gene basis, only one gene is changed on average. In addition to changing the mutation probability, the mutation step was changed from adding and/or subtracting a percentage of a max perturbation parameter value to and/or from the gene value to sampling a value from a Gaussian distribution where the mean is the gene's current value before mutation and the standard deviation is one fourth the valid range of the gene/parameter $\left (\frac{1-(-1)}{4}=0.5 \right )$. Here the standard deviation is one fourth the range and thus most of the sampled values will be within two standard deviations of the mean or rather the gene value. Any sampled gene value that was outside the valid range of $[-1,1]$ was clipped to the valid range. Note that by going with this new mutation step schema, the max perturbation parameter to the genetic algorithm could be removed thereby lessening the amount of parameters---to the genetic algorithm---that need to be experimented/played with. 

\begin{table}[H]
\footnotesize
\begin{tabular}{ >{\columncolor[gray]{0.8}} l | l }
\hline
Number of Elite Offspring                                            & 2                                                                 \\ \hline
Roulette Selection using Actual Fitness                              & False                                                             \\ \hline
Roulette Selection using Rank Fitness                                & True                                                              \\ \hline
Crossover Probability                                                & 0.8                                                               \\ \hline
Crossover Type                                                       & One Point Crossover                                               \\ \hline
Mutation Probability                                                 & $\frac{1}{n \ genes}=\frac{1}{41} = 0.024390244$                  \\ \hline
Mutation Scope                                                       & Gene Level                                                        \\ \hline
Mutation Step                                                        & Gaussian Dist. Sample $\in N(\mu=gene \ value, \sigma = 0.5)$     \\ \hline
Max Perturbation                                                     & N/A                                                       	 \\ \hline
Sequential Crossover and Mutation                                    & True                                                      	 \\ \hline
Offspring not Crossed/Mutated Allowed in New Generation              & True                                                     	 \\ \hline
Fitness Credit for Hitting the Ball                                  & N/A                                                      	 \\ \hline
Fitness Credit for Following the Ball                                & N/A                                                      	 \\ \hline
Fitness Credit for not Following the Ball                            & N/A                                                      	 \\ \hline
Fitness Credit for Staying in the Path of the Ball                   & 1.0                                                      	 \\ \hline
\end{tabular}
\caption{Here you see the genetic algorithm parameters for experiment two.}
\label{tab:exp2}
\end{table}

\subsection{Experiment three: self-adaptation of crossover and mutation probabilities with the crossover and mutation operators working in parallel.}

Experiment three involved self-adaptation of the crossover and mutation probabilities. The genetic algorithm was run for 100 generations with each generation's average fitness being recorded. Once every 10th generation, the current population's top performing genome was set aside. These 10 every-10th-generation-top-performers were run for five rounds each with the time they kept the ball-in-play being recorded per each one of their individual five rounds. Once an every-10th-generation-top-performer was done with their five rounds, the average of their ball-in-play time was calculated and recorded. The platform used was the laptop. The genetic algorithm parameters used are listed in Table \ref{tab:exp3}.

As outlined in \cite{self_adapt}, the crossover and mutation probabilities self-adapt based on the crossover and mutation operators' ability to produce fitter genomes from one generation to the next. To facilitate the self-adaptation, the crossover and mutation operators' viability to produce fitter and fitter offspring needed to be tracked from one generation to another. As the operators were being evaluated on their own accord, they were not allowed to interfere with each others offspring and thus they were not run in sequence but rather were run in parallel. With each new generation produced, elite offspring were marked accordingly as well as offspring produced by crossover and offspring produced my mutation. For those offspring that were created by crossover, their parents' weighted mean fitness was annotated along with the offspring. This weighted mean fitness was calculated based on the crossover point thereby determining what percentage of genes came from one parent and what percentage of genes came from the other parent. For example, let there be 41 genes per genome and let the crossover point be gene 10. Thus the offspring received 10 genes from parent one and received 31 genes from parent two. Therefore the offspring's weighted mean parent fitness would be calculated as $\bar{x} = \left ( p_1.fitness * \frac{10}{41} \right ) + \left (p_2.fitness * \frac{(41-10)}{41} \right)$. For those offspring created via mutation, their parent fitness was whatever the fitness was of the pre-mutated genome. 

With the genomes marked as to how they were created and their parent fitness annotated, now when it came time to produce a new population, each operators' ability to produce fitter offspring could be tracked and calculated. Once a population was evaluated, all genomes created by crossover were used to calculate the average crossover progress and all genomes created by mutation were used to calculate the average mutation progress. If the crossover progress average was greater than the mutation progress average then the crossover probability would be adjusted up and the mutation probability would be adjusted down. Alternatively, if the crossover progress average was less than the mutation progress average then the crossover probability would be adjusted down and the mutation probability would be adjusted up. If the crossover progress average equaled the mutation progress average then neither were adjusted. In other words, for example, if the crossover operator produced fitter genomes greater than the mutation operator did for the last generation, then the probability of creating offspring via crossover would be increased and the probability of creating offspring via mutation would be decreased for the creation of the next generation. Adjustment of the crossover and mutation probabilities were adjusted by the adjustment parameter. Let the adjustment parameter be denoted as $\theta$. Here $\theta$ was self-adjusted as well as outlined in \cite{self_adapt}. Once the probabilities were adjusted they were clamped to the range $[0.001,1.0]$. With a minimum probability of $0.001$, there would always be some crossover and/or mutation albeit not much. See Figure \ref{fig:sa}. Note that to insure a level playing field, both the crossover and mutation probabilities were set to an initial value of $0.5$ before the start of the genetic algorithm.

\begin{figure}[H]
\begin{equation*}
\boxed{
\begin{aligned}
Begin \ Self\!-\!adaptation: & \\
& Population \ P_i \ has \ been \ evaluated. \\
& cCount = mCount = 0. \\
& CP_{sum} = MP_{sum} = 0. \\
& \overline{CP} = \overline{MP} = 0. \\
& For \ j = 1 \ to \ population \ size: \\
& \quad\quad If \ genome_j \ created \ by \ crossover: \\
& \quad\quad\quad\quad CP_{sum} \longleftarrow CP_{sum} + ( genome_j.fitness - genome_j.parentFitness ). \\
& \quad\quad\quad\quad cCount \longleftarrow cCount + 1. \\
& \quad\quad Else \ if \ genome_j \ created \ by \ mutation: \\
& \quad\quad\quad\quad MP_{sum} \longleftarrow MP_{sum} + (genome_j.fitness - genome_j.parentFitness). \\
& \quad\quad\quad\quad mCount \longleftarrow mCount + 1. \\
& \overline{CP} = \frac{CP_{sum}}{cCount}. \\
& \overline{MP} = \frac{MP_{sum}}{mCount}. \\
& If \ P_i \ Best \ Fitness > P_i \ Worst \ Fitness: \\
& \quad\quad \theta \longleftarrow 0.01 * \frac{P_i \ Best \ Fitness - P_i \ Mean \ Fitness }{P_i \ Best \ Fitness - P_i \ Worst \ Fitness }.\\
& Else  \ if \ P_i \ Best \ Fitness = P_i \ Mean \ Fitness: \\
& \quad\quad \theta \longleftarrow 0.01  \ . \\
& If \ \overline{CP} > \overline{MP}: \\
& \quad\quad Crossover \ Prob. \longleftarrow Crossover \ Prob. + \theta. \\
& \quad\quad Mutation \ Prob. \longleftarrow Mutation \ Prob. - \theta. \\
& Else \ if \ \overline{CP} < \overline{MP}: \\
& \quad\quad Crossover \ Prob. \longleftarrow Crossover \ Prob. - \theta. \\
& \quad\quad Mutation \ Prob. \longleftarrow Mutation \ Prob. + \theta. \\
& Clamp \ Crossover \ Prob. \ to \ [0.001, 1.0]. \\
& Clamp \ Mutation \ Prob. \ to \ \ [0.001, 1.0]. \\
\end{aligned}
}
\end{equation*}
\caption{Here you see the self-adaptation algorithm.}
\label{fig:sa}
\end{figure}

\vspace{3mm}

Unlike previous experiments, the mutation probability was not on a gene-by-gene basis but rather on a whole genome-by-genome basis. Every time through the population creation loop, a random float value was sampled from a uniform distribution between $0.0$ and $1.0$. If this random float value was less than or equal to the mutation probability, every gene in the selected genome was mutated using the Gaussian distribution mutation step method outlined earlier. However, the standard deviation was set to the mutation probability instead of it being statically set to $0.5$ as before. The reason being that a high mutation probability would give way to a large mutation step (since the standard deviation would be large) allowing for large random leaps around the fitness landscape while a low mutation probability would give way to a small mutation step (since the standard deviation would be small) allowing for a more finely tuned search as the population converges to the optimum in the fitness landscape.

\begin{table}[H]
\footnotesize
\begin{tabular}{ >{\columncolor[gray]{0.8}} l | l }
\hline
Number of Elite Offspring                                            & 2                                                                            \\ \hline
Roulette Selection using Actual Fitness                              & False                                                                        \\ \hline
Roulette Selection using Rank Fitness                                & True                                                                         \\ \hline
Self-adaptation                                                      & True                                                                         \\ \hline
Initial Crossover Probability                                        & 0.5                                                                          \\ \hline
Minimum Crossover Probability                                        & 0.001                                                                        \\ \hline
Crossover Type                                                       & One Point Crossover                                                          \\ \hline
Initial Mutation Probability                                         & 0.5                                                                          \\ \hline
Minimum Mutation Probability                                         & 0.001                                                                        \\ \hline
Mutation Scope                                                       & Genome Level                                                                 \\ \hline
Mutation Step                                                        & Gaussian Dist. Sample $\in N(\mu=gene \ val., \sigma = mutation \ prob.)$    \\ \hline
Sequential Crossover and Mutation                                    & False                                                      	            \\ \hline
Offspring not Crossed/Mutated Allowed in New Generation              & False                                                     	            \\ \hline
Fitness Credit for Staying in the Path of the Ball                   & 1.0                                                      	            \\ \hline
\end{tabular}
\caption{Here you see the genetic algorithm parameters for experiment three.}
\label{tab:exp3}
\end{table}

\subsection{Experiment four: static crossover and mutation probabilities with the crossover and mutation operators working in parallel.}

Experiment four had an identical setup to experiment three with the exception that the crossover and mutation probability was statically set, through out the experiment, to the crossover and mutation probabilities arrived at after the 100 generational run of the genetic algorithm in experiment three. As with the previous experiments, the genetic algorithm was run for 100 generations with each generation's average fitness being recorded. Once every 10th generation, the current population's top performing genome was set aside. These 10 every-10th-generation-top-performers were run for five rounds each with the time they kept the ball-in-play being recorded per each one of their individual five rounds. Once an every-10th-generation-top-performer was done with their five rounds, the average of their ball-in-play time was calculated and recorded. The platform used was the laptop. The genetic algorithm parameters used are listed in Table \ref{tab:exp4}.

Experiment four as well as experiment five were devised as comparisons to experiment three. The hypothesis was that self-adaptation, along with non-interfering operators, would have the fastest average fitness growth rate among the three.

\begin{table}[H]
\footnotesize
\begin{tabular}{ >{\columncolor[gray]{0.8}} l | l }
\hline
Number of Elite Offspring                                            & 2                                                                            \\ \hline
Roulette Selection using Actual Fitness                              & False                                                                        \\ \hline
Roulette Selection using Rank Fitness                                & True                                                                         \\ \hline
Self-adaptation                                                      & False                                                                        \\ \hline
Crossover Probability                                                & 0.7816                                                                       \\ \hline
Crossover Type                                                       & One Point Crossover                                                          \\ \hline
Mutation Probability                                                 & 0.2184                                                                       \\ \hline
Mutation Scope                                                       & Genome Level                                                                 \\ \hline
Mutation Step                                                        & Gaussian Dist. Sample $\in N(\mu=gene \ val., \sigma = mutation \ prob.)$    \\ \hline
Sequential Crossover and Mutation                                    & False                                                      	            \\ \hline
Offspring not Crossed/Mutated Allowed in New Generation              & False                                                     	            \\ \hline
Fitness Credit for Staying in the Path of the Ball                   & 1.0                                                      	            \\ \hline
\end{tabular}
\caption{Here you see the genetic algorithm parameters for experiment four.}
\label{tab:exp4}
\end{table}

\subsection{Experiment five: static crossover and mutation probabilities with the crossover and mutation operators working in sequence.}

Experiment five had an identical setup as experiment four with the exception that the crossover and mutation operators were run in sequence instead of in parallel. By running the operators in sequence, one operator could disrupt the offspring/solutions created by the other operator. As with the other experiments, the genetic algorithm was run for 100 generations with each generation's average fitness being recorded. Once every 10th generation, the current population's top performing genome was set aside. These 10 every-10th-generation-top-performers were run for five rounds each with the time they kept the ball-in-play being recorded per each one of their individual five rounds. Once an every-10th-generation-top-performer was done with their five rounds, the average of their ball-in-play time was calculated and recorded. The platform used was the laptop. The genetic algorithm parameters used are listed in Table \ref{tab:exp5}.

Experiment five as well as experiment four were devised as comparisons to experiment three. The hypothesis was that self-adaptation, along with non-interfering operators, would have the fastest average fitness growth rate among the three.

\begin{table}[H]
\footnotesize
\begin{tabular}{ >{\columncolor[gray]{0.8}} l | l }
\hline
Number of Elite Offspring                                            & 2                                                                              \\ \hline
Roulette Selection using Actual Fitness                              & False                                                                          \\ \hline
Roulette Selection using Rank Fitness                                & True                                                                           \\ \hline
Self-adaptation                                                      & False                                                                          \\ \hline
Crossover Probability                                                & 0.7816                                                                         \\ \hline
Crossover Type                                                       & One Point Crossover                                                            \\ \hline
Mutation Probability                                                 & 0.2184                                                                         \\ \hline
Mutation Scope                                                       & Gene Level                                                                     \\ \hline
Mutation Step                                                        & Gaussian Dist. Sample $\in N(\mu=gene \ val., \sigma = mutation \ prob.)$      \\ \hline
Sequential Crossover and Mutation                                    & True                                                      	              \\ \hline
Offspring not Crossed/Mutated Allowed in New Generation              & False                                                     	              \\ \hline
Fitness Credit for Staying in the Path of the Ball                   & 1.0                                                      	              \\ \hline
\end{tabular}
\caption{Here you see the genetic algorithm parameters for experiment five.}
\label{tab:exp5}
\end{table}

\subsection{Experiment six: fitness and ball-in-play upper bound.}

The setup for experiment six was identical to that of experiment three with the exception that no evolution ever took place (crossover and mutation probability was set to $0.0$). With no evolution taking place, the number-of-elite-offspring parameter was set to $10$---equal to the population size---in order to keep the genetic algorithm operable. The platform used was the laptop. The genetic algorithm parameters used are listed in Table \ref{tab:exp6}. 

The goal of experiment six was to obtain the average fitness over 100 generations, the fitness of every 10th generation top performer, and the average ball-in-play time of five rounds per every 10th generation top performer where the paddle always performed the correct movement no matter the state of the paddle and the ball at any time during any round. In other words, the goal of experiment six was to obtain a fitness upper bound and a ball-in-play time upper bound utilizing the same fitness function as was used in experiment three, four, and five. 

To accomplish the goal of experiment six, the neural network output was ignored and instead, the difference in heights between the paddle's center and the ball's center was used to determine the paddle's velocity magnitude and its velocity angle at every draw loop. If the difference was $\vartriangle\!y<0$ (meaning the paddle was higher up the screen than the ball) the paddle's velocity angle was set to $\theta=270.0^\circ$ and the paddle's velocity magnitude was set to $m = -1 * \vartriangle\!y * 10$. Otherwise, if the difference was $\vartriangle\!y\geq 0$ (meaning the paddle was lower down the screen than the ball) the paddle's velocity angle was set to $\theta=90.0^\circ$ and the paddle's velocity magnitude was set to $m = \vartriangle\!y * 10$. With this modification, the paddle was always in the path of the ball and thus always kept the ball from leaving the left side of the arena. At no point was the paddle ever not in the path of the ball and thus the paddle always obtained the maximum fitness possible as well as the maximum ball-in-play time possible for any particular round. The only way any round ever terminated was by the ball's velocity magnitude falling below the $100$ threshold. 

\begin{table}[H]
\footnotesize
\begin{tabular}{ >{\columncolor[gray]{0.8}} l | l }
\hline
Number of Elite Offspring                                            & 10                                                                             \\ \hline
Roulette Selection using Actual Fitness                              & False                                                                          \\ \hline
Roulette Selection using Rank Fitness                                & True                                                                           \\ \hline
Self-adaptation                                                      & False                                                                          \\ \hline
Crossover Probability                                                & 0.0                                                                            \\ \hline
Crossover Type                                                       & One Point Crossover                                                            \\ \hline
Mutation Probability                                                 & 0.0                                                                            \\ \hline
Mutation Scope                                                       & Gene Level                                                                     \\ \hline
Mutation Step                                                        & Gaussian Dist. Sample $\in N(\mu=gene \ val., \sigma = mutation \ prob.)$      \\ \hline
Sequential Crossover and Mutation                                    & False                                                      	              \\ \hline
Offspring not Crossed/Mutated Allowed in New Generation              & False                                                     	              \\ \hline
Fitness Credit for Staying in the Path of the Ball                   & 1.0                                                      	              \\ \hline
\end{tabular}
\caption{Here you see the genetic algorithm parameters for experiment six.}
\label{tab:exp6}
\end{table}

\section{Experimental Results}

\subsection{Experiment one: use of a full and partial credit granting fitness function.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp1_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment one.}
  \label{fig:exp1_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp1_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment one.}
  \label{fig:exp1_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp1_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment one.}
  \label{fig:exp1_10_tops_times}
\end{figure}

\subsection{Experiment two: use of a simplified fitness function, use of rank fitness in selection, higher crossover probability, mutation probability based on the number of genes per genome, and a Gaussian distribution sample mutation step.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp2_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment two.}
  \label{fig:exp2_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp2_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment two.}
  \label{fig:exp2_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp2_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment two.}
  \label{fig:exp2_10_tops_times}
\end{figure}

\subsection{Experiment three: self-adaptation of crossover and mutation probabilities with the crossover and mutation operators working in parallel.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp3_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment three.}
  \label{fig:exp3_avg_fit}
\end{figure}

The crossover and mutation probabilities were both initially set to $0.5$ before the start of the experiment. After the experiment was over, the genetic algorithm self-adapted the crossover probability to $0.7816$ and self-adapted the mutation probability to $0.2184$. You'll notice in Figure \ref{fig:exp3_self_adapt} that the mutation probability overtook the crossover probability at first but the two probabilities eventually diverged with mutation becoming less probable and crossover become more probable as the genetic algorithm produced fitter generations. This outcome is almost the exact opposite of the outcome shown in \cite{self_adapt}.

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp3_self_adapt.png}
  \caption{Here you see the self-adaptation of the crossover and mutation probabilities for experiment three.}
  \label{fig:exp3_self_adapt}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp3_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment three.}
  \label{fig:exp3_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp3_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment three.}
  \label{fig:exp3_10_tops_times}
\end{figure}

\subsection{Experiment four: static crossover and mutation probabilities with the crossover and mutation operators working in parallel.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp4_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment four.}
  \label{fig:exp4_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp4_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment four.}
  \label{fig:exp4_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp4_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment four.}
  \label{fig:exp4_10_tops_times}
\end{figure}

\subsection{Experiment five: static crossover and mutation probabilities with the crossover and mutation operators working in sequence.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp5_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment five.}
  \label{fig:exp5_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp5_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment five.}
  \label{fig:exp5_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp5_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment five.}
  \label{fig:exp5_10_tops_times}
\end{figure}

\subsection{Experiment six: fitness and ball-in-play upper bound.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp6_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment six.}
  \label{fig:exp6_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp6_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment six.}
  \label{fig:exp6_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp6_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment six.}
  \label{fig:exp6_10_tops_times}
\end{figure}

\section{Comparative Analysis}

\subsection{Experiment one and two.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp12_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment one and two.}
  \label{fig:exp12_10_tops_times}
\end{figure}

\subsection{Experiment three, four, and five.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp345_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment three, four, and five.}
  \label{fig:exp345_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp345_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment three, four, and five.}
  \label{fig:exp345_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp345_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment three, four, and five.}
  \label{fig:exp345_10_tops_times}
\end{figure}

\subsection{Experiment four, five, and six.}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp46_avg_fit.png}
  \caption{Here you see the average fitness over 100 generations for experiment four and six.}
  \label{fig:exp46_avg_fit}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp56_10_tops.png}
  \caption{Here you see the fitness of the every 10th generation top performer for experiment five and six.}
  \label{fig:exp56_10_tops}
\end{figure}

\begin{figure}[H]  
  \centering
  \includegraphics[width=1\textwidth]{figures/exp56_10_tops_times.png}
  \caption{Here you see the average ball-in-play time (in seconds) of five rounds per every 10th generation top performer for experiment five and six.}
  \label{fig:exp56_10_tops_times}
\end{figure}

\section{Conclusion}

The genetic algorithm developed for SimPL should prove to be a robust basis for the genetic algorithm needed to solve a harder problem of tuning a 3D physics engine (project BBAutoTune). The principles and techniques of evolutionary algorithms learned during the SimPL project will certainly carry over to the more difficult project, BBAutoTune. And while the problem domain of SimPL and BBAutoTune are quite different, the problems faced and worked-out during the development of SimPL should alleviate the problems faced while developing BBAutoTune. As the results show, the genetic algorithm for SimPL performed well, producing neural network weight solutions that had the paddle keeping the ball in the area for almost a minute. Had it not been for the round termination criteria of the ball's velocity magnitude dropping below $100$, most of the paddles (with high fitnesses) would have kept the ball in the arena indefinitely. Thus, the goal to learn about and to cultivate a genetic algorithm capable of tuning parameters with respect to a fitness landscape was certainly accomplished.     

\bibliographystyle{plain}
\bibliography{bibliography.bib}

\end{document}
